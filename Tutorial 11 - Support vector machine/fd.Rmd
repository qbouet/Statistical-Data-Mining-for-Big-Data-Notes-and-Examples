
---
title: "Classification and Regression Trees"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading Required Libraries
The `tree` library is used to construct classification and regression trees.

```{r}
library(tree)
```

We first use classification trees to analyze the `Carseats` data set. In these data, `Sales` is a continuous variable, and so we begin by recoding it as a binary variable. We use the `ifelse()` function to create a variable, `High`, which takes on a value of `"Yes"` if the `Sales` variable exceeds 8, and `"No"` otherwise.

```{r}
library(ISLR2)
attach(Carseats)
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
```

Finally, we use the `data.frame()` function to merge `High` with the rest of the `Carseats` data.

```{r}
Carseats <- data.frame(Carseats, High)
```

We now use the `tree()` function to fit a classification tree in order to predict `High` using all variables except `Sales`.

```{r}
tree.carseats <- tree(High ~ . - Sales, Carseats)
```

The `summary()` function lists the variables used in the tree, the number of terminal nodes, and the (training) error rate.

```{r}
summary(tree.carseats)
```

One of the most attractive properties of trees is that they can be graphically displayed. We use the `plot()` function to display the tree structure and the `text()` function to display node labels.

```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

If we type the name of the tree object, R prints output corresponding to each branch of the tree.

```{r}
tree.carseats
```

## Splitting Data into Training and Test Sets
In order to evaluate the performance of a classification tree, we split the data into training and test sets.

```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]
High.test <- High[-train]
tree.carseats <- tree(High ~ . - Sales, Carseats, subset = train)
```

We use the `predict()` function to predict the class labels for the test data.

```{r}
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
(104 + 50) / 200
```

## Pruning the Tree
Next, we perform cross-validation using the `cv.tree()` function to determine the optimal level of tree complexity.

```{r}
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
cv.carseats
```

We plot the error rate as a function of both size and the cost-complexity parameter `k`.

```{r}
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```

We now prune the tree to obtain the nine-node tree.

```{r}
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

We check how the pruned tree performs on the test data.

```{r}
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
(97 + 58) / 200
```

If we increase the value of `best`, we obtain a larger pruned tree with lower classification accuracy.

```{r}
prune.carseats <- prune.misclass(tree.carseats, best = 14)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
(102 + 52) / 200
```
