---
title: "Tutorial 2"
author: "Quentin Bouet"
date: "2024-08-07"
output:
  pdf_document:
    highlight: breezedark
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lecture: PCA Theory

During the lecture this week, I skipped over the theory behind PCA. It is however crucial to grasp how PCA
works. Since students in MA3405 have varying levels of mathematical expertise, we will take a step-by-step
approach to cover the theory of PCA thoroughly.

We will use the NCI60 data in ISLR2 package for demonstration. We will start by loading the dataset on the
working directory,

```{r}
#install.packages('ISLR2')
library(ISLR2)
nci.labs <- NCI60$labs
table(nci.labs)

nci.data <- NCI60$data
dim(nci.data)
```
NCI60 data contain gene expression level of 6830 genes from 64 cancer cell lines. The format is a list containing
two elements: `data` and `labs. data` is a 64 by 6830 matrix of the expression values while `labs` is a vector
listing the cancer types for the 64 cell lines.

Principal components are orthogonal vectors which explains variation of the data. The goal of a PCA is to
find a new set of variables (i.e. smaller than original number of variables) that best describe the variation in the dataset. This is achieved via eigendecomposition of the covariance matrix. Therefore, the first step is to
find the covariance matrix of centred nci.data with cov funciton. TO save computation time, we will only
use the expression of first 500 genes,

```{r}
nci.sub<- nci.data[, 1:500] #use first 500 genes
nci.sub.scale<-scale(nci.sub, scale = TRUE) # centering
var.cov<-cov(nci.sub) # covariance
```

The next step is to eigenden composition of the variance-covariance matrix. To do this in R, we need to use
eigen function in matlib library,

```{r}
library(matlib)
e_decom<-eigen(var.cov)
#e_decom
```

The function returns two elements, the first element is the eigenvalue of var-cov matris, while the second
element is the eigenvector. We can see amount of variation explained by dividing each eigenvalue by the
number of sample -1,

```{r}
e_var<-e_decom$values/(nrow(nci.sub.scale)-1)
#We can convert this into percentage
e_var_per<-e_var/sum(e_var)
```

We can use cumsum function to see cumulative variance explained by each eigenvector,

```{r}
cumsum(e_var_per)
```

The first 35 eigenvectors explained around 90% variation, and the 62 eigenvectors explained 99% of vairation.
Let’s produce a scree plot,

```{r}
library(ggplot2)
pp<-data.frame("PC"=c(1:12), PER=e_var_per[1:12])
pp
ggplot(pp, aes(x = PC, y = PER)) +
geom_col(width = 0.5, color = "black") +
xlab("Principal component") +
ylab("Percentage of variation (%)") +
theme_classic()
```

Let’s use princomp function in R to run PCA,

```{r}
pca<-prcomp(nci.sub, center = TRUE)
summary(pca)
```

summary shows amount of variance explained by each component, these are the same as the values we got
using eigen decomposition (pp). There is however some variation in the factor loading, this is because
princomp uses Single value decomposition instead of eigen decomposition.

# Labs

## 12.5.1 Principal Components Analysis

In this lab, we perform PCA on the USArrests data set, which is part of
the base R package. The rows of the data set contain the 50 states, in
alphabetical order.

```{r}
library(ISLR)
data("USArrests")
states <- row.names(USArrests)
states
```

The columns of the data set contain the four variables.

```{r}
names(USArrests)
```

We first briefly examine the data. We notice that the variables have vastly
different means.

```{r}
apply(USArrests, 2, mean)
```

Note that the `apply()` function allows us to apply a function—in this case,
the `mean()` function to each row or column of the data set. The second
input here denotes whether we wish to compute the mean of the rows, 1,
or the columns, 2. We see that there are on average three times as many
rapes as murders, and more than eight times as many assaults as rapes.
We can also examine the variances of the four variables using the `apply()`
function.

```{r}
apply(USArrests, 2, var)
```

Not surprisingly, the variables also have vastly different variances: the
UrbanPop variable measures the percentage of the population in each state
living in an urban area, which is not a comparable number to the number of
rapes in each state per 100,000 individuals. If we failed to scale the
variables before performing PCA, then most of the principal components
that we observed would be driven by the Assault variable, since it has by
far the largest mean and variance. Thus, it is important to standardize the
variables to have mean zero and standard deviation one before performing
PCA.

We now perform principal components analysis using the `prcomp()` function, 
which is one of several functions in R that perform PCA.

```{r}
pr.out <- prcomp(USArrests, scale = TRUE)
```

By default, the prcomp() function centers the variables to have mean zero.
By using the option scale = TRUE, we scale the variables to have standard
deviation one. The output from `prcomp()` contains a number of useful quan-
tities.

```{r}
names(pr.out)
```

The center and scale components correspond to the means and standard
deviations of the variables that were used for scaling prior to implementing
PCA.

```{r}
pr.out$center
pr.out$scale
```

The rotation matrix provides the principal component loadings; each col-
umn of pr.out$rotation contains the corresponding principal component
loading vector.

```{r}
# This function names it the rotation matrix, because when we matrix-multiply the
# X matrix by pr.out$rotation, it gives us the coordinates of the data in the rotated
# coordinate system. These coordinates are the principal component scores.
pr.out$rotation
```

We see that there are four distinct principal components. This is to be
expected because there are in general min(n - 1, p) informative principal
components in a data set with n observations and p variables.
Using the `prcomp()` function, we do not need to explicitly multiply the
data by the principal component loading vectors in order to obtain the
principal component score vectors. Rather the 50 × 4 matrix x has as its
columns the principal component score vectors. That is, the kth column is
the kth principal component score vector.

```{r}
dim(pr.out$x)
```

We can plot the first two principal components as follows:

```{r}
biplot(pr.out, scale = 0)
```

The scale = 0 argument to `biplot()` ensures that the arrows are scaled to `biplot()`
represent the loadings; other values for scale give slightly different biplots
with different interpretations.
Notice that this figure is a mirror image of Figure 12.1. Recall that
the principal components are only unique up to a sign change, so we can
reproduce Figure 12.1 by making a few small changes:

```{r}
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale = 0)
```

The `prcomp()` function also outputs the standard deviation of each principal 
component. For instance, on the USArrests data set, we can access
these standard deviations as follows:

```{r}
pr.out$sdev
```

The variance explained by each principal component is obtained by squaring these:

```{r}
pr.var <- pr.out$sdev^2
pr.var
```

To compute the proportion of variance explained by each principal component, 
we simply divide the variance explained by each principal component
by the total variance explained by all four principal components:

```{r}
pve <- pr.var / sum(pr.var)
pve
```

We see that the first principal component explains 62.0 % of the variance
in the data, the next principal component explains 24.7 % of the variance, 
and so forth. We can plot the PVE explained by each component, as well
as the cumulative PVE, as follows:

```{r}
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0, 1),
     type = "b")
plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0, 1),
     type = "b")
```

The result is shown in Figure 12.3. Note that the function `cumsum()` computes the
cumulative sum of the elements of a numeric vector. For instance:

```{r}
a <- c(1, 2, 8, -3)
cumsum(a)
```

## 12.5.4 NCI60 Data Example

Unsupervised techniques are often used in the analysis of genomic data.
In particular, PCA and hierarchical clustering are popular tools. We illus-
trate these techniques on the NCI60 cancer cell line microarray data, which
consists of 6,830 gene expression measurements on 64 cancer cell lines.

```{r}
library(ISLR2)
nci.labs <- NCI60$labs
nci.data <- NCI60$data
```

Each cell line is labeled with a cancer type, given in nci.labs. We do not
make use of the cancer types in performing PCA and clustering, as these
are unsupervised techniques. But after performing PCA and clustering, we
will check to see the extent to which these cancer types agree with the
results of these unsupervised techniques.

The data has 64 rows and 6,830 columns.

```{r}
dim(nci.data)
```
We begin by examining the cancer types for the cell lines.

```{r}
nci.labs [1:4]
```

```{r}
table(nci.labs)
```

### PCA on the NCI60 Data

We first perform PCA on the data after scaling the variables (genes) to
have standard deviation one, although one could reasonably argue that it
is better not to scale the genes.

```{r}
pr.out <- prcomp(nci.data , scale = TRUE)
```

We now plot the first few principal component score vectors, in order to
visualize the data. The observations (cell lines) corresponding to a given
cancer type will be plotted in the same color, so that we can see to what
extent the observations within a cancer type are similar to each other. We
first create a simple function that assigns a distinct color to each element
of a numeric vector. The function will be used to assign a color to each of
the 64 cell lines, based on the cancer type to which it corresponds.

```{r}
Cols <- function(vec){
                      cols <- rainbow(length(unique(vec)))
                      return(cols[as.numeric(as.factor(vec))])
                     }
```

Note that the `rainbow()` function takes as its argument a positive integer, 
and returns a vector containing that number of distinct colors. We now can
plot the principal component score vectors.

```{r}
par(mfrow = c(1, 2))
plot(pr.out$x[, 1:2], 
     col = Cols(nci.labs), 
     pch = 19,
     xlab = "Z1", 
     ylab = "Z2")
plot(pr.out$x[, c(1, 3)], 
     col = Cols(nci.labs), 
     pch = 19,
     xlab = "Z1", 
     ylab = "Z3")
```

The resulting plots are shown in Figure 12.17. On the whole, cell lines
corresponding to a single cancer type do tend to have similar values on the
first few principal component score vectors. This indicates that cell lines
from the same cancer type tend to have pretty similar gene expression
levels.

We can obtain a summary of the proportion of variance explained (PVE)
of the first few principal components using the `summary()` method for a
prcomp object (we have truncated the printout):

```{r}
summary(pr.out)
```

Using the `plot()` function, we can also plot the variance explained by the
first few principal components.

```{r}
plot(pr.out)
```

Note that the height of each bar in the bar plot is given by squaring the
corresponding element of `pr.out$sdev`. However, it is more informative to
plot the PVE of each principal component (i.e. a scree plot) and the cu-
mulative PVE of each principal component. This can be done with just a
little work.

```{r}
pve <- 100 * pr.out$sdev^2 / sum(pr.out$sdev ^2)
par(mfrow = c(1, 2))
plot(pve, 
     type = "o", 
     ylab = "PVE",
     xlab = "Principal Component", 
     col = "blue")
plot(cumsum(pve), 
     type = "o", 
     ylab = "Cumulative PVE",
     xlab = "Principal Component", 
     col = "brown3")
```

(Note that the elements of pve can also be computed directly from the summary,
`summary(pr.out)$importance[2, ]`, and the elements of `cumsum(pve)`
are given by `summary(pr.out)$importance[3, ]`.) The resulting plots are
shown in Figure 12.18. We see that together, the first seven principal components 
explain around 40 % of the variance in the data. This is not a huge
amount of the variance. However, looking at the scree plot, we see that
while each of the first seven principal components explain a substantial
amount of variance, there is a marked decrease in the variance explained
by further principal components. That is, there is an elbow in the plot
after approximately the seventh principal component. This suggests that
there may be little benefit to examining more than seven or so principal
components (though even examining seven principal components may be
difficult).

### Clustering the Observations of the NCI60 Data

We now proceed to hierarchically cluster the cell lines in the NCI60 data,
with the goal of finding out whether or not the observations cluster into
distinct types of cancer. To begin, we standardize the variables to have 
mean zero and standard deviation one. As mentioned earlier, this step is
optional and should be performed only if we want each gene to be on the
same scale.

```{r}
sd.data <- scale(nci.data)
```

We now perform hierarchical clustering of the observations using complete,
single, and average linkage. Euclidean distance is used as the dissimilarity
measure.

```{r}
#par(mfrow = c(1, 3))
data.dist <- dist(sd.data)
plot(hclust(data.dist), #complete linkage is default
     xlab = "", 
     sub = "", 
     ylab = "",
     labels = nci.labs,
     main = "Complete Linkage", 
     cex=0.5)
plot(hclust(data.dist, method = "average"),
            labels = nci.labs,
            main = "Average Linkage",
            xlab = "",
            sub = "",
            ylab = "",
            cex=0.5)
plot(hclust(data.dist, method = "single"),
            labels = nci.labs,
            main = "Single Linkage",
            xlab = "", 
            sub = "", 
            ylab = "", 
            cex=0.5)
```

The results are shown in Figure 12.19. We see that the choice of linkage
certainly does affect the results obtained. Typically, single linkage will tend
to yield trailing clusters: very large clusters onto which individual observa-
tions attach one-by-one. On the other hand, complete and average linkage
tend to yield more balanced, attractive clusters. For this reason, complete
and average linkage are generally preferred to single linkage. Clearly cell
lines within a single cancer type do tend to cluster together, although the
clustering is not perfect. We will use complete linkage hierarchical cluster-
ing for the analysis that follows.

We can cut the dendrogram at the height that will yield a particular
number of clusters, say four:

```{r}
hc.out <- hclust(dist(sd.data))
hc.clusters <- cutree(hc.out , 4)
table(hc.clusters , nci.labs)
```

There are some clear patterns. All the leukemia cell lines fall in cluster 3,
while the breast cancer cell lines are spread out over three different clusters.
We can plot the cut on the dendrogram that produces these four clusters:

```{r}
par(mfrow = c(1, 1))
plot(hc.out , labels = nci.labs)
abline(h = 139, col = "red")
```

The `abline()` function draws a straight line on top of any existing plot
in R. The argument h = 139 plots a horizontal line at height 139 on the
dendrogram; this is the height that results in four distinct clusters. It is easy
to verify that the resulting clusters are the same as the ones we obtained
using cutree(hc.out, 4).

Printing the output of hclust gives a useful brief summary of the object:

```{r}
hc.out
```

We claimed earlier in Section 12.4.2 that K-means clustering and hier-
archical clustering with the dendrogram cut to obtain the same number
of clusters can yield very different results. How do these NCI60 hierarchical
clustering results compare to what we get if we perform K-means clustering
with K = 4?

```{r}
set.seed (2)
km.out <- kmeans(sd.data , 4, nstart = 20)
km.clusters <- km.out$cluster
table(km.clusters , hc.clusters)
```

We see that the four clusters obtained using hierarchical clustering and K-
means clustering are somewhat different. Cluster 4 in K-means clustering is
identical to cluster 3 in hierarchical clustering. However, the other clusters
differ: for instance, cluster 2 in K-means clustering contains a portion of
the observations assigned to cluster 1 by hierarchical clustering, as well as
all of the observations assigned to cluster 2 by hierarchical clustering.
Rather than performing hierarchical clustering on the entire data matrix,
we can simply perform hierarchical clustering on the first few principal
component score vectors, as follows:

```{r}
hc.out <- hclust(dist(pr.out$x[, 1:5]))
plot(hc.out , labels = nci.labs ,
main = "Hier. Clust. on First Five Score Vectors")
table(cutree(hc.out , 4), nci.labs)
```

Not surprisingly, these results are different from the ones that we obtained
when we performed hierarchical clustering on the full data set. Sometimes
performing clustering on the first few principal component score vectors
can give better results than performing clustering on the full data. In this
situation, we might view the principal component step as one of denois-
ing the data. We could also perform K-means clustering on the first few
principal component score vectors rather than the full data set.

# Questions

## Exercise 8, Chapter 12

In Section 12.2.3, a formula for calculating the *proportion of variance explained* 
(PVE) was given in Equation 12.10. We also saw that the PVE can be obtained using 
the sdev output of the `prcomp()` function.

On the USArrests data, calculate PVE in two ways:

  a. Using the sdev output of the `prcomp()` function, as was done in
Section 12.2.3.

```{r}
#load data
#install.packages('ISLR') #note: may need to install.packages in the console!
library(ISLR)
data("USArrests")

# Note: The variance explained by each principal component:
# pr.var = pr.out$sdev^2

pr.out = prcomp(USArrests, scale = TRUE)
pve =100*pr.out$sdev^2 / sum(pr.out$sdev^2)
pve

plot(pve,
     type = "o",
     ylab = "PVE",
     xlab = "Principal Component",
     col = "blue")
```

  b. By applying Equation 12.10 directly. That is, use the `prcomp()`
function to compute the principal component loadings. Then,
use those loadings in Equation 12.10 to obtain the PVE.

![](1.png)

```{r}
# Obtain loadings from prcomp() function
# The loadings make up the principal component loading vector
loadings <- pr.out$rotation 
# Scale dataset just to make sure the data we use is consistent
USArrests2 <- scale(USArrests)

# Convert dataset into matrix, square each value in matrix, and sum them up 
# to get the denominator of the equation
sumvalue <- sum(as.matrix(USArrests2)^2)

# Multiply these two matrices and then square
# Note: - "%*%" is matrix multiplication
#       - xij is "as.matrix(USArrests2)"
#       - phijm is "loadings"
#       - only works as xij times phijm (not interchangeable, 
#         results in non-conformable arguments error if written as 
#         "loadings %*% as.matrix(USArrests2)")
num <- (as.matrix(USArrests2)%*%loadings)^2

#calculate the column value for num matrix
colvalue<-c()
for (i in 1:length(num[1,])){
  colvalue[i]<-sum(num[,i])
}
#calculate new pve
pve <- 100*colvalue/sumvalue
pve
```

These two approaches should give the same results.

*Hint: You will only obtain the same results in (a) and (b) if the
same data is used in both cases. For instance, if in (a) you performed
`prcomp()` using centered and scaled variables, then you must center
and scale the variables before applying Equation 12.10 in (b).*

## Sparrows

Download the `sparrow2.csv` data from LearnJCU. This dataset consists of seven morphological
variables taken from 1026 sparrows. The seven variables were:

-  wingcrd = wingcord
-  flating = flattened wing
-  tarsus = leg
-  head = bill tip to back of skull
-  culmen = beak length
-  nalopsi = bill tip to nostril
-  weight

Conduct exploratory data analysis to check if there is correlation between variables.

```{r}
Sparrows2 <- read.csv("Sparrows2.csv")
dim(Sparrows2)
variables <- names(Sparrows2)
variables
```

\textcolor{red}{We first briefly examine the data. We notice that the variables have vastly different means.}

```{r}
apply(Sparrows2, 2, mean)
```

\textcolor{red}{We can also examine the variances of the four variables
using the apply() function.}

```{r}
apply(Sparrows2, 2, var)
```
\textcolor{red}{We now perform principal components analysis using the prcomp() function, which is one of several functions in R that perform PCA. It is important to standardize the variables to have mean
zero and standard deviation one before performing PCA. By default, the prcomp() function centers the variables to have mean zero. By using the option scale = TRUE, we scale the variables to have standard deviation one. The output from prcomp() contains a number of useful quantities.}


```{r}
pr.out <- prcomp(Sparrows2, scale = TRUE)
names(pr.out)
```

\textcolor{red}{The center and scale components correspond to the means and standard deviations of the variables that
were used for scaling prior to implementing PCA.}

```{r}
pr.out$center
pr.out$scale
```

\textcolor{red}{The rotation matrix provides the principal component loadings; each column of pr.out$rotation contains the corresponding principal component loading vector.}


```{r}
# This function names it the rotation matrix, because when we matrix-multiply the
# X matrix by pr.out$rotation, it gives us the coordinates of the data in the rotated
# coordinate system. These coordinates are the principal component scores.
pr.out$rotation
```
\textcolor{red}{We see that there are seven distinct principal components. This is to be expected because there are in general min(n - 1, p) informative principal components in a data set with n observations and p variables. Using
the prcomp() function, we do not need to explicitly multiply the data by the principal component loading
vectors in order to obtain the principal component score vectors. Rather the 1026 × 7 matrix x has as its
columns the principal component score vectors. That is, the kth column is the kth principal component
score vector.}

```{r}
dim(pr.out$x)
```
\textcolor{red}{We can plot the first two principal components as follows with the biplot() function:}

```{r}
biplot(pr.out, 
       scale = 0, 
       cex = 0.7, 
       xlab="First Principle Component (PC1)",
       ylab="Second Principle Component (PC2)")
```

\textcolor{red}{We want the arrow things to be in the positive areas. SO let's redo it.}
```{r}
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale = 0)
```

\textcolor{red}{The black numbers represent the scores for the first two principal components. The
orange arrows indicate the first two principal component loading vectors (with axes
on the top and right).}

\textcolor{red}{Here we can compute the proportion of variance explained by each principal component:}

```{r}
pr.var <- pr.out$sdev^2
pve <- pr.var / sum(pr.var)
pve
```

\textcolor{red}{We see that the first principal component explains 64.7 % of the variance in the data, the next principal component explains 16.3 % of the variance, and so forth. We can plot the PVE explained by each component,
as well as the cumulative PVE, as follows:}

```{r}
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0, 1),
     type = "b")
plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0, 1),
     type = "b")
```


\textcolor{red}{However, looking at the scree plot, we see that while each of the first three principal components explain a substantial amount of variance, there is a marked decrease in the variance explained by further principal components. That is, there is an elbow in the plot after approximately the third principal component. This suggests that there may be little benefit to examining more than three or so principal components.}

Perform PCA and answer the following questions:

1. How much variation is explained in the (i) first (ii) and (iii) principal component analysis?

\textcolor{red}{The first principal component explains 64.7 % of the variance in the data, the next principal component explains 16.3 % of the variance, and the third principal component explains 6.3% of the variance.}

2. How many principal components do you recommend using? Why?

\textcolor{red}{Looking at the scree plot, we see that while each of the first three principal components explain a substantial amount of variance, there is a marked decrease in the variance explained by further principal components. That is, there is an elbow in the plot after approximately the third principal component. This suggests that there may be little benefit to examining more than three or so principal components.}

3. Can you describe the first two principal components?

\textcolor{red}{We see that the first loading vector places approximately
equal weight on bill tip to back of skull, beak length and bill tip to nostril, 
but with much less weight on flattened wing and wingcord. 
Hence this component roughly corresponds to a measure of beak/bill-related variables. 
The second loading vector places most of its weight on the wing-related features much less 
weight on the other three features. Hence, this component roughly corresponds 
to measures of wing-related variables. Overall, we see that the wing-related 
variables are located close to each other, and that 
the beak/bill-related variables are far from the others.}

4. Interpret any interesting features in the biplot.

\textcolor{red}{The biplot (This figure is known as a biplot, because it
displays both the principal component scores and the principal component loadings)
indicates that the two wing-related variables, flattened wing and wingcord are strongly correlated
with each other. On the other hand, the beak/bill-related variables (bill tip to back of skull, beak length and bill tip to nostril) are correlated with each other too.}
